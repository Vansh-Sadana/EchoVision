{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9fd3b9d-7f38-4fbb-898c-3a2798a0b261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-genai\n",
      "  Using cached google_genai-1.63.0-py3-none-any.whl.metadata (53 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from google-genai) (4.12.1)\n",
      "Collecting google-auth<3.0.0,>=2.47.0 (from google-auth[requests]<3.0.0,>=2.47.0->google-genai)\n",
      "  Using cached google_auth-2.48.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Collecting pydantic<3.0.0,>=2.9.0 (from google-genai)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from google-genai) (2.32.5)\n",
      "Collecting tenacity<9.2.0,>=8.2.3 (from google-genai)\n",
      "  Using cached tenacity-9.1.4-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai)\n",
      "  Using cached websockets-15.0.1-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from google-genai) (4.15.0)\n",
      "Collecting distro<2,>=1.7.0 (from google-genai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting sniffio (from google-genai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting cryptography>=38.0.3 (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai)\n",
      "  Using cached cryptography-46.0.5-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: certifi in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.9.0->google-genai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.9.0->google-genai)\n",
      "  Using cached pydantic_core-2.41.5-cp311-cp311-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.9.0->google-genai)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.6.3)\n",
      "Requirement already satisfied: cffi>=2.0.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (2.0.0)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai)\n",
      "  Using cached pyasn1-0.6.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: pycparser in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from cffi>=2.0.0->cryptography>=38.0.3->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (3.0)\n",
      "Using cached google_genai-1.63.0-py3-none-any.whl (724 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached google_auth-2.48.0-py3-none-any.whl (236 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "Using cached tenacity-9.1.4-py3-none-any.whl (28 kB)\n",
      "Using cached websockets-15.0.1-cp311-cp311-win_amd64.whl (176 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached cryptography-46.0.5-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached pyasn1-0.6.2-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: websockets, typing-inspection, tenacity, sniffio, pydantic-core, pyasn1, distro, annotated-types, rsa, pydantic, pyasn1-modules, cryptography, google-auth, google-genai\n",
      "Successfully installed annotated-types-0.7.0 cryptography-46.0.5 distro-1.9.0 google-auth-2.48.0 google-genai-1.63.0 pyasn1-0.6.2 pyasn1-modules-0.4.2 pydantic-2.12.5 pydantic-core-2.41.5 rsa-4.9.1 sniffio-1.3.1 tenacity-9.1.4 typing-inspection-0.4.2 websockets-15.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc58717-cd2f-4d32-a183-cd8674999dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "f = open(\"keys/.gemini.txt\")\n",
    "key = f.read()\n",
    "\n",
    "client = genai.Client(api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5e36443-c936-486c-b205-846c388d44d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image captures a dense, nighttime traffic scene on a major urban thoroughfare. It is a powerful representation of urban life and the challenges of commuting in a megacity.\n",
      "\n",
      "### Key Elements of the Image:\n",
      "\n",
      "*   **Traffic Density:** The central focus is the overwhelming volume of vehicles. The road is packed across multiple lanes with various types of transportation, including private cars (sedans and SUVs), large public buses, a heavy delivery truck in the foreground, and at least one motorcycle weaving through the lanes.\n",
      "*   **Perspective and Composition:** The photograph is taken from a high-angle perspective, likely from a pedestrian overpass. This viewpoint emphasizes the scale of the traffic, using the lanes and the elevated structure on the right as leading lines that draw the eye toward the distant city horizon.\n",
      "*   **Lighting and Atmosphere:** The scene is defined by high contrast. The deep shadows of the night are punctured by the bright, varied lights of the city: white and yellow headlights, red taillights, glowing billboards, and streetlamps. This creates a vibrant but somewhat chaotic atmosphere.\n",
      "*   **Urban Infrastructure:** \n",
      "    *   On the **right**, there is a massive concrete elevated structure, likely a light rail transit (LRT/MRT) track or a highway flyover. \n",
      "    *   Prominent **advertising** is visible, specifically a large billboard for \"The Olive Place,\" which provides a specific geographical clue.\n",
      "    *   On the **left**, there is a structured area with railings and glass, suggesting a major bus stop or transit hub.\n",
      "\n",
      "### Location Clues:\n",
      "The billboard for \"The Olive Place\" mentions **\"407 Shaw Blvd, Mandaluyong City.\"** Combined with the characteristic style of the buses, the concrete barriers, and the specific density of the traffic, this identifies the location as **EDSA (Epifanio de los Santos Avenue) in Metro Manila, Philippines**. This road is world-renowned for its extreme traffic congestion.\n",
      "\n",
      "### Overall Impression:\n",
      "The image conveys a sense of \"organized chaos.\" While the vehicles are confined to lanes, the sheer volume suggests a slow-moving, grinding commute. It captures the restless energy of a city that never fully stops, highlighted by the glowing trails of light against the dark urban backdrop.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "with open('images/image_2.jpeg', 'rb') as f:\n",
    "  image_bytes = f.read()\n",
    "\n",
    "context = [\n",
    "    types.Part.from_bytes(\n",
    "        data=image_bytes,\n",
    "        mime_type='image/jpeg',\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-3-flash-preview\",\n",
    "    contents=context,\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab4087d5-ea02-4cad-a407-41eb48b76f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: soundfile in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: librosa in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: filelock in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from datasets) (3.24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from datasets) (2.3.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from datasets) (23.0.1)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from datasets) (4.67.3)\n",
      "Requirement already satisfied: xxhash in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: packaging in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from datasets) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from soundfile) (2.0.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from librosa) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from librosa) (0.63.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from librosa) (1.17.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from librosa) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from librosa) (1.5.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from librosa) (1.9.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: pycparser in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from cffi>=1.0->soundfile) (3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.23.1)\n",
      "Requirement already satisfied: llvmlite<0.47,>=0.46.0dev0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from numba>=0.51.0->librosa) (0.46.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from pooch>=1.1->librosa) (4.9.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: colorama in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: typer>=0.23.1 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.23.1)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from typer>=0.23.1->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from typer>=0.23.1->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (14.3.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from typer>=0.23.1->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\ai enablement\\image_narration_system\\.env_image_narration\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.1->typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets soundfile librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5666727-c58e-47c7-812b-4c0d25309e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6172e706bc84537a4a3c042ba1cc028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.1.weight to fine_acoustics.lm_heads.0.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.2.weight to fine_acoustics.lm_heads.1.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.3.weight to fine_acoustics.lm_heads.2.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.4.weight to fine_acoustics.lm_heads.3.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.5.weight to fine_acoustics.lm_heads.4.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.6.weight to fine_acoustics.lm_heads.5.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie fine_acoustics.input_embeds_layers.7.weight to fine_acoustics.lm_heads.6.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "narrator = pipeline(\n",
    "    task=\"text-to-speech\",\n",
    "    model=\"suno/bark-small\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aace56e-4e3b-48ae-b51f-f83120bfc016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n",
      "Both `max_new_tokens` (=768) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=60) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=54) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "narrated_text = narrator(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2db867a4-495b-438f-96a5-82e567b325c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Array audio input must be a 1D or 2D array",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Audio \u001b[38;5;28;01mas\u001b[39;00m IPythonAudio\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mIPythonAudio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnarrated_text\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnarrated_text\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msampling_rate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\AI Enablement\\image_narration_system\\.env_image_narration\\Lib\\site-packages\\IPython\\lib\\display.py:131\u001b[39m, in \u001b[36mAudio.__init__\u001b[39m\u001b[34m(self, data, filename, url, embed, rate, autoplay, normalize, element_id)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mrate must be specified when data is a numpy array or list of audio samples.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[43mAudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_make_wav\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\AI Enablement\\image_narration_system\\.env_image_narration\\Lib\\site-packages\\IPython\\lib\\display.py:153\u001b[39m, in \u001b[36mAudio._make_wav\u001b[39m\u001b[34m(data, rate, normalize)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwave\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     scaled, nchan = \u001b[43mAudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_validate_and_normalize_with_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    155\u001b[39m     scaled, nchan = Audio._validate_and_normalize_without_numpy(data, normalize)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\AI Enablement\\image_narration_system\\.env_image_narration\\Lib\\site-packages\\IPython\\lib\\display.py:184\u001b[39m, in \u001b[36mAudio._validate_and_normalize_with_numpy\u001b[39m\u001b[34m(data, normalize)\u001b[39m\n\u001b[32m    182\u001b[39m     data = data.T.ravel()\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mArray audio input must be a 1D or 2D array\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    186\u001b[39m max_abs_value = np.max(np.abs(data))\n\u001b[32m    187\u001b[39m normalization_factor = Audio._get_normalization_factor(max_abs_value, normalize)\n",
      "\u001b[31mValueError\u001b[39m: Array audio input must be a 1D or 2D array"
     ]
    }
   ],
   "source": [
    "from IPython.display import Audio as IPythonAudio\n",
    "\n",
    "IPythonAudio(\n",
    "    narrated_text[\"audio\"][0],\n",
    "    rate=narrated_text[\"sampling_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb66565-75af-4350-a226-13fd71655166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
